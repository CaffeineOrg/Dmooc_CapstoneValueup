{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7316883",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from google.cloud import storage\n",
    "import textrank\n",
    "\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f8d9580",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Hurricane Florence came by while I was working on statquest, dark clouds filled the sky, but that didn't stop statquest stats code. Hello, I'm just armor and welcome, statquest. Today. We're going to be talking about some machine learning, fundamentals, bias, and variance. And they're going to be clearly explained. Imagine we measured the weight and height of a bunch of mice and plotted the data on a graph. Light my skin to be short. And heavier, mice tend to be taller. But after a certain weight mice, don't get any taller just more obese. Given the stator, we would like to predictor Mouse height, given its weight. For example, if you told me your mouse weigh this much. Then we might predict that the mouse is this tall. Ideally, we would know the exact mathematical formula that describes the relationship between weight and height. But in this case, we don't know the formula. So we're going to use to machine learning methods to approximate this relationship. However, I'll leave the true relationship curve in the figure for reference. The first thing we do is put the data into two sets, one for training, the machine learning algorithms and one for testing them. The blue dots are the training set and the green dots are the testing set. Here's just the training set. The first machine learning algorithm that we will use is linearregression AKA least-squares. Linear regression, fits a straight line to the training set. Note, the straight line doesn't have the flexibility to accurately. Replicate The Ark in the true relationship. No matter how we try to fit the line. He will never curve. The straight-line will never capture the true relationship between weight and height. No matter how well we fit it to the training set. The inability for a machine learning method, like linearregression to capture. The true relationship is called by us because the straight line can't be curved like the true relationship. It has a relatively large amount of bias. Another machine learning method, might fit a squiggly line to the training set. The squiggly line is super flexible and hugs. The training set along the archive, the true relationship. Because the squiggly line can handle the Ark in the true relationship between weight and height. It has very little bias. We can compare how well the straight line in the squiggly line. Hit the training set by calculating their sums of squares. In other words, we measure the distances from the fit lines to the data squared them and add them up. They are squared, so that negative distances do not cancel out, positive distances. Notice how the squiggly line fits the data. So well that the distance is between the line and the data are all 0. In the contest to see whether the straight-line fits the training set better than the squiggly line. The squiggly line winds. But remember so far, we've only calculated the sums of squares for the training set. We also have a testing set. Now, let's calculate the sums of squares for the testing set. In the contest to see whether the straight-line fits the testing set better than the squiggly line. The straight line winds. Even though the squiggly line did a great job hitting the training set. It did a terrible job, hitting the testing set. In machine learning lingo, the difference in fits between data sets is called variance. The squiggly line has lobias since it is flexible and can adapt to the curve in the relationship between weight and height. But the squiggly line has high variability because it results in vastly different sums of squares for different data sets. In other words, it's hard to predict how well the squiggly line will perform with future data sets. It might do well sometimes and other times it might do terribly. In contrast, the straight line has relatively High by us since it cannot capture the curve in the relationship between weight and height, but the straight line has relatively low variance because the sums of squares are very similar for different data sets. In other words, the straight-line might only give good predictions and not grape predictions, but they will be consistently. Good predictions, bam. Oh, no, terminology alert, because the squiggly line fits the training set really well, but not the testing set. We say that the squiggly line is over fit in machine learning. The ideal algorithm has Low by us and can accurately model the true relationship. And it has low variability by producing consistent predictions across different data sets. This is down by finding The Sweet Spot between a simple model and a complex model. Oh, no, another terminology Alert 3. Commonly used methods for finding the Sweet Spot between simple and complicated. Models are regular ization boost and a bagging. The stats Quest on random Forest. Show an example of bagging in action and we'll talk about regularization and posting in future. Statquest double bam. Gray, we've made it to the end of another exciting. Statquest. If you like this data Quest and want to see more. Please subscribe. And if you want to support statquest, well, please consider buying one or two of my original songs. All right, until next time Quest on.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3379a582",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('statquest', 0.5071), ('stats', 0.5045), ('regression', 0.483), ('bias', 0.4503), ('variance', 0.4373), ('variability', 0.4289), ('data', 0.3263), ('learning', 0.31), ('mathematical', 0.3032), ('stator', 0.2892), ('algorithms', 0.2772), ('plotted', 0.2731), ('linearregression', 0.2714), ('algorithm', 0.2625), ('predict', 0.2561), ('predictor', 0.2544), ('hurricane', 0.2524), ('calculated', 0.2515), ('random', 0.2487), ('accurately', 0.2475)]\n",
      "[('statquest', 0.5071), ('stats', 0.5045), ('regression', 0.483), ('bias', 0.4503), ('variance', 0.4373), ('variability', 0.4289), ('data', 0.3263), ('learning', 0.31), ('mathematical', 0.3032), ('stator', 0.2892)]\n",
      "['statquest', 'stats', 'regression', 'bias', 'variance', 'variability', 'data', 'learning', 'mathematical', 'stator']\n"
     ]
    }
   ],
   "source": [
    "stopwords_path = '../../text/stop_words_english.txt'\n",
    "with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
    "    stop_words = f.readlines()\n",
    "    stop_words = [word.strip() for word in stop_words]\n",
    "    \n",
    "keywords = textrank.get_keywords(text, word_num=20, \n",
    "#                                  stopwords_list=stop_words\n",
    "                                )\n",
    "pro_keywords = textrank.postprocess_keywords(keywords)\n",
    "keywords_only = [word for (word, _) in pro_keywords]\n",
    "print(keywords)\n",
    "print(pro_keywords)\n",
    "print(keywords_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17dac247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['statquest', 'stat', 'regress', 'bia', 'varianc', 'variabl', 'data', 'learn', 'mathemat', 'stator']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "keywords_stem = [stemmer.stem(word) for word in keywords_only]\n",
    "print(keywords_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eb6035",
   "metadata": {},
   "source": [
    "# Keyword Highlighting\n",
    "script 내에서 keyword와 일치 또는 품사만 다른 단어가 있으면 highlighting 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86ffff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list_text = text.split()\n",
    "for idx, word_in_text in enumerate(words_list_text):\n",
    "    word_stem = stemmer.stem(word_in_text)\n",
    "    if word_stem in keywords_stem:\n",
    "        words_list_text[idx] = '<span>' + word_in_text + '</span>'\n",
    "        \n",
    "text = ' '.join(words_list_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7c5b534",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hurricane Florence came by while I was working on statquest, dark clouds filled the sky, but that didn't stop <span>statquest</span> <span>stats</span> code. Hello, I'm just armor and welcome, statquest. Today. We're going to be talking about some machine learning, fundamentals, bias, and variance. And they're going to be clearly explained. Imagine we measured the weight and height of a bunch of mice and plotted the <span>data</span> on a graph. Light my skin to be short. And heavier, mice tend to be taller. But after a certain weight mice, don't get any taller just more obese. Given the stator, we would like to predictor Mouse height, given its weight. For example, if you told me your mouse weigh this much. Then we might predict that the mouse is this tall. Ideally, we would know the exact <span>mathematical</span> formula that describes the relationship between weight and height. But in this case, we don't know the formula. So we're going to use to machine <span>learning</span> methods to approximate this relationship. However, I'll leave the true relationship curve in the figure for reference. The first thing we do is put the <span>data</span> into two sets, one for training, the machine <span>learning</span> algorithms and one for testing them. The blue dots are the training set and the green dots are the testing set. Here's just the training set. The first machine <span>learning</span> algorithm that we will use is linearregression AKA least-squares. Linear regression, fits a straight line to the training set. Note, the straight line doesn't have the flexibility to accurately. Replicate The Ark in the true relationship. No matter how we try to fit the line. He will never curve. The straight-line will never capture the true relationship between weight and height. No matter how well we fit it to the training set. The inability for a machine <span>learning</span> method, like linearregression to capture. The true relationship is called by us because the straight line can't be curved like the true relationship. It has a relatively large amount of bias. Another machine <span>learning</span> method, might fit a squiggly line to the training set. The squiggly line is super flexible and hugs. The training set along the archive, the true relationship. Because the squiggly line can handle the Ark in the true relationship between weight and height. It has very little bias. We can compare how well the straight line in the squiggly line. Hit the training set by calculating their sums of squares. In other words, we measure the distances from the fit lines to the <span>data</span> squared them and add them up. They are squared, so that negative distances do not cancel out, positive distances. Notice how the squiggly line fits the data. So well that the distance is between the line and the <span>data</span> are all 0. In the contest to see whether the straight-line fits the training set better than the squiggly line. The squiggly line winds. But remember so far, we've only calculated the sums of squares for the training set. We also have a testing set. Now, let's calculate the sums of squares for the testing set. In the contest to see whether the straight-line fits the testing set better than the squiggly line. The straight line winds. Even though the squiggly line did a great job hitting the training set. It did a terrible job, hitting the testing set. In machine <span>learning</span> lingo, the difference in fits between <span>data</span> sets is called variance. The squiggly line has lobias since it is flexible and can adapt to the curve in the relationship between weight and height. But the squiggly line has high <span>variability</span> because it results in vastly different sums of squares for different <span>data</span> sets. In other words, it's hard to predict how well the squiggly line will perform with future <span>data</span> sets. It might do well sometimes and other times it might do terribly. In contrast, the straight line has relatively High by us since it cannot capture the curve in the relationship between weight and height, but the straight line has relatively low <span>variance</span> because the sums of squares are very similar for different <span>data</span> sets. In other words, the straight-line might only give good predictions and not grape predictions, but they will be consistently. Good predictions, bam. Oh, no, terminology alert, because the squiggly line fits the training set really well, but not the testing set. We say that the squiggly line is over fit in machine learning. The ideal algorithm has Low by us and can accurately model the true relationship. And it has low <span>variability</span> by producing consistent predictions across different <span>data</span> sets. This is down by finding The Sweet Spot between a simple model and a complex model. Oh, no, another terminology Alert 3. Commonly used methods for finding the Sweet Spot between simple and complicated. Models are regular ization boost and a bagging. The <span>stats</span> Quest on random Forest. Show an example of bagging in action and we'll talk about regularization and posting in future. <span>Statquest</span> double bam. Gray, we've made it to the end of another exciting. Statquest. If you like this <span>data</span> Quest and want to see more. Please subscribe. And if you want to support statquest, well, please consider buying one or two of my original songs. All right, until next time Quest on.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9437dc9",
   "metadata": {},
   "source": [
    "# Keyword Timestamps\n",
    "word timestamp 딕셔너리 돌다가 word가 keyword와 일치 또는 품사만 다른 단어라면 start_time 기록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aef67a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = [{'word':'that', 'start_time':0, 'end_time':0.1}, \n",
    "              {'word':'didn\\'t', 'start_time':0.2, 'end_time':0.3}, \n",
    "              {'word':'statquest', 'start_time':0.4, 'end_time':0.5}]\n",
    "keywords_only = ['statquest']\n",
    "keywords_stem = [stemmer.stem(word) for word in keywords_only]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01931f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_timestamps = {keyword:[] for keyword in keywords_only}\n",
    "for word_dict in timestamps:\n",
    "    word_stem = stemmer.stem(word_dict['word'])\n",
    "    if word_stem in keywords_stem:\n",
    "        keyword_ori = keywords_only[keywords_stem.index(word_stem)]\n",
    "        keyword_timestamps[keyword_ori].append(word_dict['start_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1528cf1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'statquest': [0.4]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_timestamps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
